{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>post</th>\n",
       "      <th>md5_hash</th>\n",
       "      <th>lewd</th>\n",
       "      <th>off</th>\n",
       "      <th>intention</th>\n",
       "      <th>grp</th>\n",
       "      <th>ing</th>\n",
       "      <th>group_set</th>\n",
       "      <th>statement_set</th>\n",
       "      <th>...</th>\n",
       "      <th>sampling_output_4</th>\n",
       "      <th>sampling_output_5</th>\n",
       "      <th>sampling_output_6</th>\n",
       "      <th>sampling_output_7</th>\n",
       "      <th>sampling_output_8</th>\n",
       "      <th>sampling_output_9</th>\n",
       "      <th>sampling_output_10</th>\n",
       "      <th>best_sampling_output</th>\n",
       "      <th>best_sampling_output_idx</th>\n",
       "      <th>best_sampling_output_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|startoftext|&gt; Fuc u say ? @AyyyeThatsChubb: ...</td>\n",
       "      <td>Fuc u say ? @AyyyeThatsChubb: All these hoes l...</td>\n",
       "      <td>99f1f9c8d4abca821626837048093f3b</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;|startoftext|&gt; Fuc u say ? @AyyyeThatsChubb: ...</td>\n",
       "      <td>&lt;|startoftext|&gt; Fuc u say ? @AyyyeThatsChubb: ...</td>\n",
       "      <td>&lt;|startoftext|&gt; Fuc u say ? @AyyyeThatsChubb: ...</td>\n",
       "      <td>&lt;|startoftext|&gt; Fuc u say ? @AyyyeThatsChubb: ...</td>\n",
       "      <td>&lt;|startoftext|&gt; Fuc u say ? @AyyyeThatsChubb: ...</td>\n",
       "      <td>&lt;|startoftext|&gt; Fuc u say ? @AyyyeThatsChubb: ...</td>\n",
       "      <td>&lt;|startoftext|&gt; Fuc u say ? @AyyyeThatsChubb: ...</td>\n",
       "      <td>&lt;|startoftext|&gt; Fuc u say ? @AyyyeThatsChubb: ...</td>\n",
       "      <td>6</td>\n",
       "      <td>-7.557428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  <|startoftext|> Fuc u say ? @AyyyeThatsChubb: ...   \n",
       "\n",
       "                                                post  \\\n",
       "0  Fuc u say ? @AyyyeThatsChubb: All these hoes l...   \n",
       "\n",
       "                           md5_hash  lewd  off  intention  grp  ing group_set  \\\n",
       "0  99f1f9c8d4abca821626837048093f3b     0    0          0    0    0        []   \n",
       "\n",
       "  statement_set  ...                                  sampling_output_4  \\\n",
       "0            []  ...  <|startoftext|> Fuc u say ? @AyyyeThatsChubb: ...   \n",
       "\n",
       "                                   sampling_output_5  \\\n",
       "0  <|startoftext|> Fuc u say ? @AyyyeThatsChubb: ...   \n",
       "\n",
       "                                   sampling_output_6  \\\n",
       "0  <|startoftext|> Fuc u say ? @AyyyeThatsChubb: ...   \n",
       "\n",
       "                                   sampling_output_7  \\\n",
       "0  <|startoftext|> Fuc u say ? @AyyyeThatsChubb: ...   \n",
       "\n",
       "                                   sampling_output_8  \\\n",
       "0  <|startoftext|> Fuc u say ? @AyyyeThatsChubb: ...   \n",
       "\n",
       "                                   sampling_output_9  \\\n",
       "0  <|startoftext|> Fuc u say ? @AyyyeThatsChubb: ...   \n",
       "\n",
       "                                  sampling_output_10  \\\n",
       "0  <|startoftext|> Fuc u say ? @AyyyeThatsChubb: ...   \n",
       "\n",
       "                                best_sampling_output best_sampling_output_idx  \\\n",
       "0  <|startoftext|> Fuc u say ? @AyyyeThatsChubb: ...                        6   \n",
       "\n",
       "  best_sampling_output_score  \n",
       "0                  -7.557428  \n",
       "\n",
       "[1 rows x 26 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model_name = \"gpt2-small-1337-5\"\n",
    "\n",
    "path_dev = f\"tmp/output/{model_name}/output_dev.csv\"\n",
    "path_test = f\"tmp/output/{model_name}/output_test.csv\"\n",
    "\n",
    "df_dev = pd.read_csv(path_dev)\n",
    "df_test = pd.read_csv(path_test)\n",
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4673, 4698)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_dev), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['prompt', 'post', 'md5_hash', 'lewd', 'off', 'intention', 'grp', 'ing',\n",
       "       'group_set', 'statement_set', 'group_multiple', 'statement_multiple',\n",
       "       'greedy_output', 'sampling_output_1', 'sampling_output_2',\n",
       "       'sampling_output_3', 'sampling_output_4', 'sampling_output_5',\n",
       "       'sampling_output_6', 'sampling_output_7', 'sampling_output_8',\n",
       "       'sampling_output_9', 'sampling_output_10', 'best_sampling_output',\n",
       "       'best_sampling_output_idx', 'best_sampling_output_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 1, 0, 5, 6, 2, 3, 4, 8, 9])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev[\"best_sampling_output_idx\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Lists of True Labels and Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import ast\n",
    "\n",
    "\n",
    "def get_true_labels(df: pd.DataFrame) -> Dict[str, List[int]]:\n",
    "    true_labels = {\n",
    "        \"lewd\": [int(item) for item in df[\"lewd\"].tolist()],\n",
    "        \"off\": [int(item) for item in df[\"off\"].tolist()],\n",
    "        \"intention\": [int(item) for item in df[\"intention\"].tolist()],\n",
    "        \"grp\": [int(item) for item in df[\"grp\"].tolist()],\n",
    "        \"ing\": [int(item) for item in df[\"ing\"].tolist()],\n",
    "        \"group_set\": df[\"group_set\"].tolist(),\n",
    "        \"group_multiple\": df[\"group_multiple\"].tolist(),\n",
    "        \"statement_set\": df[\"statement_set\"].tolist(),\n",
    "        \"statement_multiple\": df[\"statement_multiple\"].tolist(),\n",
    "    }\n",
    "\n",
    "    # convert strings back to lists\n",
    "    true_labels[\"group_set\"] = [\n",
    "        ast.literal_eval(item) for item in true_labels[\"group_set\"]\n",
    "    ]\n",
    "    true_labels[\"group_multiple\"] = [\n",
    "        ast.literal_eval(item) for item in true_labels[\"group_multiple\"]\n",
    "    ]\n",
    "    true_labels[\"statement_set\"] = [\n",
    "        ast.literal_eval(item) for item in true_labels[\"statement_set\"]\n",
    "    ]\n",
    "    true_labels[\"statement_multiple\"] = [\n",
    "        ast.literal_eval(item) for item in true_labels[\"statement_multiple\"]\n",
    "    ]\n",
    "\n",
    "    assert isinstance(true_labels[\"group_set\"][0], list)\n",
    "    assert isinstance(true_labels[\"group_multiple\"][0], list)\n",
    "    assert isinstance(true_labels[\"statement_set\"][0], list)\n",
    "    assert isinstance(true_labels[\"statement_multiple\"][0], list)\n",
    "\n",
    "    return true_labels\n",
    "\n",
    "\n",
    "true_labels_dev = get_true_labels(df_dev)\n",
    "true_labels_test = get_true_labels(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from utils.helper import remove_substrings\n",
    "\n",
    "\n",
    "def get_predictions(df: pd.DataFrame, pred_col: str) -> Dict[str, List[int]]:\n",
    "    model_output = df[pred_col].tolist()\n",
    "\n",
    "    predictions = {\n",
    "        \"lewd\": [1 if config.LEWD_TOKEN[1] in output else 0 for output in model_output],\n",
    "        \"off\": [1 if config.OFF_TOKEN[1] in output else 0 for output in model_output],\n",
    "        \"intention\": [\n",
    "            1 if config.INT_TOKEN[1] in output else 0 for output in model_output\n",
    "        ],\n",
    "        \"grp\": [1 if config.GRP_TOKEN[1] in output else 0 for output in model_output],\n",
    "        \"ing\": [1 if config.ING_TOKEN[1] in output else 0 for output in model_output],\n",
    "    }\n",
    "\n",
    "    group, statement = [], []\n",
    "    for output in model_output:\n",
    "        # remove original prompt from the output\n",
    "        if len(output.split(config.SEP_TOKEN)) > 1:\n",
    "            clean_output = output.split(config.SEP_TOKEN)[1]\n",
    "        else:\n",
    "            # model gave empty response\n",
    "            group.append(\"\")\n",
    "            statement.append(\"\")\n",
    "            print(\"Empty response: \", output)\n",
    "            continue\n",
    "\n",
    "        # no [SEP] token found --> empty group and statement\n",
    "        if config.HELP_SEP not in clean_output:\n",
    "            group.append(\"\")\n",
    "            statement.append(\"\")\n",
    "            continue\n",
    "\n",
    "        # if only 1 [SEP] token found --> we just guess that it is the group\n",
    "        elif clean_output.count(config.HELP_SEP) == 1:\n",
    "            # remove any classification tokens from the output\n",
    "            clean_text = remove_substrings(clean_output.split(config.HELP_SEP)[1])\n",
    "            group.append(clean_text)\n",
    "            statement.append(\"\")\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            # remove any classification tokens from the output\n",
    "            clean_grp = remove_substrings(clean_output.split(config.HELP_SEP)[1])\n",
    "            clean_stmt = remove_substrings(clean_output.split(config.HELP_SEP)[2])\n",
    "            group.append(clean_grp)\n",
    "            statement.append(clean_stmt)\n",
    "\n",
    "    predictions[\"group\"] = group\n",
    "    predictions[\"statement\"] = statement\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "list_sampling_col = [\"sampling_output_\" + str(i + 1) for i in range(10)]\n",
    "\n",
    "predictions_greedy_dev = get_predictions(df_dev, \"greedy_output\")\n",
    "predictions_greedy_test = get_predictions(df_test, \"greedy_output\")\n",
    "\n",
    "predictions_sampling_dev = get_predictions(df_dev, \"best_sampling_output\")\n",
    "predictions_sampling_test = get_predictions(df_test, \"best_sampling_output\")\n",
    "\n",
    "# predictions_sampling_dev = [get_predictions(df_dev, col) for col in list_sampling_col]\n",
    "# predictions_sampling_test = [get_predictions(df_test, col) for col in list_sampling_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Evaluation for the Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluate_categorical_variables\n",
    "\n",
    "\n",
    "def get_categorical_variable_evaluation(\n",
    "    y_true: List[int], y_pred: List[int]\n",
    ") -> Dict[str, float]:\n",
    "    return {\n",
    "        \"lewd\": evaluate_categorical_variables(y_true[\"lewd\"], y_pred[\"lewd\"]),\n",
    "        \"off\": evaluate_categorical_variables(y_true[\"off\"], y_pred[\"off\"]),\n",
    "        \"intention\": evaluate_categorical_variables(\n",
    "            y_true[\"intention\"], y_pred[\"intention\"]\n",
    "        ),\n",
    "        \"grp\": evaluate_categorical_variables(y_true[\"grp\"], y_pred[\"grp\"]),\n",
    "        \"ing\": evaluate_categorical_variables(y_true[\"ing\"], y_pred[\"ing\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "classification_result_greedy_dev = get_categorical_variable_evaluation(\n",
    "    true_labels_dev, predictions_greedy_dev\n",
    ")\n",
    "classification_result_greedy_test = get_categorical_variable_evaluation(\n",
    "    true_labels_test, predictions_greedy_test\n",
    ")\n",
    "classification_result_sampling_dev = get_categorical_variable_evaluation(\n",
    "    true_labels_dev, predictions_sampling_dev\n",
    ")\n",
    "classification_result_sampling_test = get_categorical_variable_evaluation(\n",
    "    true_labels_test, predictions_sampling_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results to JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def save_results_to_json(results: Dict[str, float], filename: str) -> None:\n",
    "    \"\"\"Save result dictionary to json file.\"\"\"\n",
    "    dir_path = os.path.dirname(filename)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "\n",
    "save_results_to_json(\n",
    "    {\n",
    "        \"greedy_dev\": classification_result_greedy_dev,\n",
    "        \"greedy_test\": classification_result_greedy_test,\n",
    "        \"sampling_dev\": classification_result_sampling_dev,\n",
    "        \"sampling_test\": classification_result_sampling_test,\n",
    "    },\n",
    "    f\"tmp/results/{model_name}/classification.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Evaluation for the Generated Group and Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get results for __greedy__ decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating predictions: 0it [00:00, ?it/s]/Users/collins/.pyenv/versions/3.11.4/envs/social_bias_frames/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Evaluating predictions: 4673it [00:00, 22732.58it/s]\n",
      "Evaluating predictions: 4673it [00:00, 22621.65it/s]\n",
      "Evaluating predictions: 4698it [00:00, 64103.92it/s]\n",
      "Evaluating predictions: 4698it [00:00, 24703.31it/s]\n",
      "Evaluating predictions: 4673it [00:00, 16213.97it/s]\n",
      "Evaluating predictions: 4673it [00:00, 16006.23it/s]\n",
      "Evaluating predictions: 4698it [00:00, 17679.40it/s]\n",
      "Evaluating predictions: 4698it [00:00, 17473.83it/s]\n"
     ]
    }
   ],
   "source": [
    "from evaluate import evaluate_generated_text\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate_wrapper(\n",
    "    predictions: List[str], references: List[List[str]]\n",
    ") -> List[Dict[str, float]]:\n",
    "    results = []\n",
    "    for pred, refs in tqdm(zip(predictions, references), desc=\"Evaluating predictions\"):\n",
    "        if not pred.strip() or not refs:  # TODO change back to 'and'\n",
    "            continue\n",
    "\n",
    "        elif not pred.strip() or not refs:\n",
    "            results.append(\n",
    "                {\n",
    "                    \"bleu\": 0.0,\n",
    "                    \"rouge\": 0.0,\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            if refs and not isinstance(refs[0], str):\n",
    "                print(\"Error in Refs: \", refs)\n",
    "                continue\n",
    "            results.append(evaluate_generated_text(pred, refs))\n",
    "\n",
    "    # get average results\n",
    "    avg_results = {}\n",
    "    for key in results[0].keys():\n",
    "        sum_value = sum(d[key] for d in results)\n",
    "        avg_results[key] = sum_value / len(results)\n",
    "\n",
    "    return avg_results\n",
    "\n",
    "\n",
    "save_results_to_json(\n",
    "    {\n",
    "        # groups\n",
    "        \"group_set_result_greedy_dev\": evaluate_wrapper(\n",
    "            predictions_greedy_dev[\"group\"], true_labels_dev[\"group_set\"]\n",
    "        ),\n",
    "        \"group_multiple_result_greedy_dev\": evaluate_wrapper(\n",
    "            predictions_greedy_dev[\"group\"], true_labels_dev[\"group_multiple\"]\n",
    "        ),\n",
    "        \"group_set_result_greedy_test\": evaluate_wrapper(\n",
    "            predictions_greedy_test[\"group\"], true_labels_test[\"group_set\"]\n",
    "        ),\n",
    "        \"group_multiple_result_greedy_test\": evaluate_wrapper(\n",
    "            predictions_greedy_test[\"group\"], true_labels_test[\"group_multiple\"]\n",
    "        ),\n",
    "        # statements\n",
    "        \"statement_set_result_greedy_dev\": evaluate_wrapper(\n",
    "            predictions_greedy_dev[\"statement\"], true_labels_dev[\"statement_set\"]\n",
    "        ),\n",
    "        \"statement_multiple_result_greedy_dev\": evaluate_wrapper(\n",
    "            predictions_greedy_dev[\"statement\"], true_labels_dev[\"statement_multiple\"]\n",
    "        ),\n",
    "        \"statement_set_result_greedy_test\": evaluate_wrapper(\n",
    "            predictions_greedy_test[\"statement\"], true_labels_test[\"statement_set\"]\n",
    "        ),\n",
    "        \"statement_multiple_result_greedy_test\": evaluate_wrapper(\n",
    "            predictions_greedy_test[\"statement\"], true_labels_test[\"statement_multiple\"]\n",
    "        ),\n",
    "    },\n",
    "    f\"tmp/results/{model_name}/greedy_generation.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get results for __sampling__ based decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating predictions: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating predictions: 4673it [00:00, 92809.98it/s]\n",
      "Evaluating predictions: 4673it [00:00, 36590.75it/s]\n",
      "Evaluating predictions: 4698it [00:00, 93412.66it/s]\n",
      "Evaluating predictions: 4698it [00:00, 33480.37it/s]\n",
      "Evaluating predictions: 4673it [00:00, 25065.81it/s]\n",
      "Evaluating predictions: 4673it [00:00, 24219.46it/s]\n",
      "Evaluating predictions: 4698it [00:00, 22705.40it/s]\n",
      "Evaluating predictions: 4698it [00:00, 22300.18it/s]\n"
     ]
    }
   ],
   "source": [
    "save_results_to_json(\n",
    "    {\n",
    "        # groups\n",
    "        \"group_set_result_sampling_dev\": evaluate_wrapper(\n",
    "            predictions_sampling_dev[\"group\"], true_labels_dev[\"group_set\"]\n",
    "        ),\n",
    "        \"group_multiple_result_sampling_dev\": evaluate_wrapper(\n",
    "            predictions_sampling_dev[\"group\"], true_labels_dev[\"group_multiple\"]\n",
    "        ),\n",
    "        \"group_set_result_sampling_test\": evaluate_wrapper(\n",
    "            predictions_sampling_test[\"group\"], true_labels_test[\"group_set\"]\n",
    "        ),\n",
    "        \"group_multiple_result_sampling_test\": evaluate_wrapper(\n",
    "            predictions_sampling_test[\"group\"], true_labels_test[\"group_multiple\"]\n",
    "        ),\n",
    "        # statements\n",
    "        \"statement_set_result_sampling_dev\": evaluate_wrapper(\n",
    "            predictions_sampling_dev[\"statement\"], true_labels_dev[\"statement_set\"]\n",
    "        ),\n",
    "        \"statement_multiple_result_sampling_dev\": evaluate_wrapper(\n",
    "            predictions_sampling_dev[\"statement\"], true_labels_dev[\"statement_multiple\"]\n",
    "        ),\n",
    "        \"statement_set_result_sampling_test\": evaluate_wrapper(\n",
    "            predictions_sampling_test[\"statement\"], true_labels_test[\"statement_set\"]\n",
    "        ),\n",
    "        \"statement_multiple_result_sampling_test\": evaluate_wrapper(\n",
    "            predictions_sampling_test[\"statement\"],\n",
    "            true_labels_test[\"statement_multiple\"],\n",
    "        ),\n",
    "    },\n",
    "    f\"tmp/results/{model_name}/sampling_generation.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "- The evaluation for the generated texts is split between reference texts that can contain duplicates and references that only contain unique sentences -> Which performs better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "social_bias_frames",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
